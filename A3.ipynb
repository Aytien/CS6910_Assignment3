{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import wandb \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs20b004\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 40\n",
    "WANDB_NOTEBOOK_NAME = 'Assignment3'\n",
    "WANDB_PROJECT_NAME = 'CS6910_A3'\n",
    "WANDB_ENTITY = 'cs20b004'\n",
    "wandb.login()\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        # self.word2index = {}\n",
    "        # self.word2count = {}\n",
    "        # self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        # self.n_words = 2  # Count SOS and EOS\n",
    "        self.letter_index = {}\n",
    "        self.letter_count = {}\n",
    "        self.index_letter = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_letters = 2  # Count SOS and EOS\n",
    "\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # for word in sentence.split(' '):\n",
    "        #     self.addWord(word)\n",
    "        for letter in word:\n",
    "            self.add_letter(letter)\n",
    "\n",
    "    def add_letter(self, letter):\n",
    "        # if word not in self.word2index:\n",
    "        #     self.word2index[word] = self.n_words\n",
    "        #     self.word2count[word] = 1\n",
    "        #     self.index2word[self.n_words] = word\n",
    "        #     self.n_words += 1\n",
    "        # else:\n",
    "        #     self.word2count[word] += 1\n",
    "        if letter not in self.letter_index:\n",
    "            self.letter_index[letter] = self.n_letters\n",
    "            self.letter_count[letter] = 1\n",
    "            self.index_letter[self.n_letters] = letter\n",
    "            self.n_letters += 1\n",
    "        else:\n",
    "            self.letter_count[letter] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words(output_lang, type = 'train', reverse = False):\n",
    "    lines = open('aksharantar_sampled/%s/%s_%s.csv' % (output_lang,output_lang,type), encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[s for s in l.split(',')] for l in lines]\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "    else:\n",
    "        pairs = [list(p) for p in pairs]\n",
    "    return pairs\n",
    "\n",
    "def init_lang(lang1, lang2, type = 'train', reverse = False):\n",
    "    pairs = read_words(lang2, type, reverse)\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    char = 'a'\n",
    "    for i in range(26):\n",
    "        input_lang.add_letter(char)\n",
    "        char = chr(ord(char) + 1)\n",
    "    char = '\\u0900'\n",
    "    for i in range(128):\n",
    "        output_lang.add_letter(char)\n",
    "        char = chr(ord(char) + 1)\n",
    "\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_LENGTH:  40\n"
     ]
    }
   ],
   "source": [
    "pairs = read_words('mar',type = 'train')\n",
    "for p in pairs:\n",
    "    MAX_LENGTH = max(MAX_LENGTH, len(p[1]))\n",
    "print(\"MAX_LENGTH: \", MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def indexesFromSentence(lang, sentence):\n",
    "#     return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "def word_to_index(lang, word):\n",
    "    return [lang.letter_index[letter] for letter in word]\n",
    "\n",
    "\n",
    "# def tensorFromSentence(lang, sentence):\n",
    "#     indexes = indexesFromSentence(lang, sentence)\n",
    "#     indexes.append(EOS_token)\n",
    "#     return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def word_to_tensor(lang, word):\n",
    "    indexes = word_to_index(lang, word)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "# def tensorsFromPair(pair):\n",
    "#     input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "#     target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "#     return (input_tensor, target_tensor)\n",
    "\n",
    "def pair_to_tensor(input_lang,output_lang, pair):\n",
    "    input_tensor = word_to_tensor(input_lang, pair[0])\n",
    "    target_tensor = word_to_tensor(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, type = 'gru', nonlinearity = 'tanh'):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.type = type\n",
    "        if self.type == 'gru':\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'lstm':\n",
    "            self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'rnn':\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers = num_layers)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        if self.type == 'gru':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.type == 'lstm':\n",
    "            output, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
    "        elif self.type == 'rnn':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "        return output, hidden, cell\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers , 1, self.hidden_size, device=device)\n",
    "    \n",
    "    def init_cell(self):\n",
    "        return self.initHidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, output_size, num_layers, nonlinearity = 'tanh', dropout_p = 0.1, type = 'gru'):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        # self.dropout_p = dropout_p\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        # self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.num_layers = num_layers\n",
    "        self.type = type\n",
    "        if self.type == 'gru':\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'lstm':\n",
    "            self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'rnn':\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers = num_layers)\n",
    "            \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # output = self.dropout(output)\n",
    "        output = F.relu(output)\n",
    "        if self.type == 'gru':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.type == 'lstm':\n",
    "            output, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
    "        elif self.type == 'rnn':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "        \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, cell\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "    \n",
    "        \n",
    "    def init_cell(self):\n",
    "        return self.initHidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, output_size, num_layers, nonlinearity = 'tanh', dropout_p=0.1, max_length=MAX_LENGTH, type = 'gru'):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(self.output_size, self.embedding_size)\n",
    "        self.attn = nn.Linear(self.hidden_size + self.embedding_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size + self.embedding_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.type = type\n",
    "        \n",
    "        if self.type == 'gru':\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'lstm':\n",
    "            self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'rnn':\n",
    "            self.rnn = nn.RNN(hidden_size, hidden_size, num_layers = num_layers)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        if self.type == 'gru':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.type == 'lstm':\n",
    "            output, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
    "        elif self.type == 'rnn':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, cell, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "        \n",
    "    def init_cell(self):\n",
    "        return self.initHidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transliterator():\n",
    "    def __init__(self, encoder_hp, decoder_hp, attn = True):\n",
    "        self.encoder = EncoderRNN(\n",
    "                        encoder_hp['input_size'],\n",
    "                        encoder_hp['embedding_size'],\n",
    "                        encoder_hp['hidden_size'], \n",
    "                        encoder_hp['num_layers'], \n",
    "                        type = encoder_hp['type']).to(device)\n",
    "        self.attn = attn\n",
    "        if attn:\n",
    "            self.decoder = AttnDecoderRNN(\n",
    "                            decoder_hp['hidden_size'], \n",
    "                            decoder_hp['embedding_size'],\n",
    "                            decoder_hp['output_size'], \n",
    "                            decoder_hp['num_layers'], \n",
    "                            type = decoder_hp['type']).to(device)\n",
    "        else:\n",
    "            self.decoder = DecoderRNN(\n",
    "                            decoder_hp['hidden_size'], \n",
    "                            decoder_hp['embedding_size'],\n",
    "                            decoder_hp['output_size'], \n",
    "                            decoder_hp['num_layers'],  \n",
    "                            type = decoder_hp['type']).to(device)\n",
    "\n",
    "    def train(self, input_tensor, target_tensor, max_length=MAX_LENGTH):\n",
    "        encoder_hidden = self.encoder.initHidden()\n",
    "        encoder_cell = self.encoder.init_cell()\n",
    "\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, self.encoder.hidden_size, device=device)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden, encoder_cell = self.encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)\n",
    "        decoder_cell = encoder_cell[encoder_cell.shape[0] - 1].unsqueeze(0)\n",
    "\n",
    "        decoded_word = ''\n",
    "        for i in range(self.decoder.num_layers - 1):\n",
    "            decoder_hidden = torch.cat((decoder_hidden, encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)), 0)\n",
    "            decoder_cell = torch.cat((decoder_cell, encoder_cell[encoder_cell.shape[0] - 1].unsqueeze(0)), 0)\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n",
    "        # print(encoder_outputs.shape)\n",
    "        if use_teacher_forcing:\n",
    "            for di in range(target_length):\n",
    "                if self.attn:\n",
    "                    decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  \n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "                decoded_word += self.output_lang.index_letter[topi.item()]\n",
    "                decoder_input = target_tensor[di]\n",
    "        else:\n",
    "            for di in range(target_length):\n",
    "                if self.attn:\n",
    "                    decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_cell, decoder_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  \n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "                decoded_word += self.output_lang.index_letter[topi.item()]\n",
    "                \n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        \n",
    "        return loss.item() / target_length, decoded_word \n",
    "    \n",
    "    def fit( self, train_io_pair, val_io_pair, input_lang, output_lang, n_epochs,optimizer = 'adam', criterion = nn.NLLLoss(), print_every=1000, learning_rate=0.0001, teacher_forcing_ratio = 0.5, use_wanb = False):\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "\n",
    "        if(optimizer == 'adam'):\n",
    "            self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=learning_rate)\n",
    "            self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=learning_rate)\n",
    "        elif(optimizer == 'sgd'):\n",
    "            self.encoder_optimizer = optim.SGD(self.encoder.parameters(), lr=learning_rate)\n",
    "            self.decoder_optimizer = optim.SGD(self.decoder.parameters(), lr=learning_rate)\n",
    "            \n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.n_epochs = n_epochs\n",
    "        self.print_every = print_every\n",
    "        \n",
    "        self.all_losses = []\n",
    "        self.total_loss = 0\n",
    "\n",
    "        # for i in range(len(train_io_pair)):\n",
    "        #     train_io_pair[i] = pair_to_tensor(train_io_pair[i])\n",
    "        training_pairs = [pair_to_tensor(self.input_lang, self.output_lang, train_io_pair[i]) for i in range(len(train_io_pair))]\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        for epc in range(1,n_epochs + 1):\n",
    "            correct = 0\n",
    "            for inp in range(len(train_io_pair)):\n",
    "                input_tensor = training_pairs[inp][0]\n",
    "                target_tensor = training_pairs[inp][1]\n",
    "                loss, decoder_output = self.train(input_tensor, target_tensor)\n",
    "                self.total_loss += loss\n",
    "                self.all_losses.append(loss)\n",
    "                if(decoder_output == train_io_pair[inp][1]):\n",
    "                    correct += 1\n",
    "            train_acc = self.eval(train_io_pair)\n",
    "            train_accs.append(train_acc)\n",
    "            val_acc = self.eval(val_io_pair)\n",
    "            val_accs.append(val_acc)\n",
    "            print(\"Epoch: \", epc, \" Loss: \", self.total_loss / len(train_io_pair),\n",
    "                  \"Training Accuracy: \", correct/len(train_io_pair),\n",
    "                  \"Validation Accuracy: \", val_acc)\n",
    "            self.total_loss = 0\n",
    "            if(use_wanb):\n",
    "                wandb.log({\n",
    "                    \"epoch\" : epc,\n",
    "                    \"train_loss\" : self.total_loss / len(train_io_pair),\n",
    "                    \"Training Accuracy\": train_acc, \n",
    "                    \"Validation Accuracy\": val_acc\n",
    "                    })\n",
    "            if len(val_accs) > 2 and val_accs[-1] < val_accs[-2]:\n",
    "                break\n",
    "            if len(train_accs) > 2 and train_accs[-1] < train_accs[-2]:\n",
    "                break\n",
    "        \n",
    "        torch.save(self.encoder.state_dict(), f'Saved_models/val_acc_{val_accs[-1]}encoder.pth')\n",
    "        torch.save(self.decoder.state_dict(), f'Saved_models/val_acc_{val_accs[-1]}decoder.pth')\n",
    "\n",
    "    def eval(self, io_pairs):\n",
    "        correct = 0\n",
    "        for i in range(len(io_pairs)):\n",
    "            output, _ = self.predict(io_pairs[i][0])\n",
    "            if output == io_pairs[i][1]:\n",
    "                correct += 1\n",
    "        return correct / len(io_pairs)\n",
    "    \n",
    "    def predict(self, input_word, max_length=MAX_LENGTH):\n",
    "        with torch.no_grad():\n",
    "            input_tensor = word_to_tensor(self.input_lang, input_word)\n",
    "            input_length = input_tensor.size()[0]\n",
    "            encoder_hidden = self.encoder.initHidden()\n",
    "            encoder_cell = self.encoder.init_cell()\n",
    "            encoder_outputs = torch.zeros(max_length, self.encoder.hidden_size, device=device)\n",
    "\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden, encoder_cell = self.encoder(input_tensor[ei],\n",
    "                                                        encoder_hidden, encoder_cell)\n",
    "                encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "            decoder_hidden = encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)\n",
    "            decoder_cell = encoder_cell[encoder_cell.shape[0] - 1].unsqueeze(0)\n",
    "\n",
    "            decoded_word = ''\n",
    "            for i in range(self.decoder.num_layers - 1):\n",
    "                decoder_hidden = torch.cat((decoder_hidden, encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)), 0)\n",
    "                decoder_cell = torch.cat((decoder_cell, encoder_cell[encoder_cell.shape[0] - 1].unsqueeze(0)), 0)\n",
    "\n",
    "            decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "            for di in range(max_length):\n",
    "                if self.attn:\n",
    "                    decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "                    decoder_attentions[di] = decoder_attention.data\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden, decoder_cell = self.decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell)\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                if topi.item() == EOS_token:\n",
    "                    # decoded_words.append('<EOS>')\n",
    "                    # decoded_word += '<EOS>'\n",
    "                    break\n",
    "                else:\n",
    "                    # decoded_words.append(output_lang.letter_index[topi.item()])\n",
    "                    decoded_word += self.output_lang.index_letter[topi.item()]\n",
    "                    # print(output_lang.index_letter[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            return decoded_word, decoder_attentions[:di + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  3.742086087481468 Training Accuracy:  0.0 Validation Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, train_pairs = init_lang( 'eng','mar', type = 'train', reverse = False)\n",
    "validation_pairs = read_words('mar', type = 'valid')\n",
    "# print(input_lang.n_letters)\n",
    "encoder_hp = {\n",
    "    'input_size': input_lang.n_letters, \n",
    "    'embedding_size': 64, \n",
    "    'hidden_size': 512, \n",
    "    'num_layers': 1, \n",
    "    'type': 'gru'}\n",
    "decoder_hp = {\n",
    "    'hidden_size': 512, \n",
    "    'embedding_size': 64, \n",
    "    'output_size': output_lang.n_letters, \n",
    "    'num_layers': 1, \n",
    "    'type': 'lstm'}\n",
    "\n",
    "model = Transliterator(encoder_hp=encoder_hp, decoder_hp=decoder_hp, attn = True)\n",
    "\n",
    "model.fit(train_pairs[:10], validation_pairs, input_lang, output_lang, optimizer='adam', n_epochs = 1, learning_rate = 0.0003, teacher_forcing_ratio = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 7zf323te\n",
      "Sweep URL: https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: h2zzgf21 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_num_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_type: gru\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_num_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_type: gru\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.6\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Aditya Patil\\Desktop\\IITM\\CS6910\\Assignment3\\wandb\\run-20230508_004358-h2zzgf21</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/h2zzgf21' target=\"_blank\">vague-sweep-1</a></strong> to <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/h2zzgf21' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/h2zzgf21</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  1.5430266266492045 Training Accuracy:  0.0 Validation Accuracy:  0.14013671875\n",
      "Epoch:  2  Loss:  1.2746000974679506 Training Accuracy:  0.0 Validation Accuracy:  0.187744140625\n",
      "Epoch:  3  Loss:  1.2286960538624323 Training Accuracy:  0.0 Validation Accuracy:  0.1787109375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▇█</td></tr><tr><td>Validation Accuracy</td><td>▁█▇</td></tr><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train_loss</td><td>▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>0.16338</td></tr><tr><td>Validation Accuracy</td><td>0.17871</td></tr><tr><td>epoch</td><td>3</td></tr><tr><td>train_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vague-sweep-1</strong> at: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/h2zzgf21' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/h2zzgf21</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230508_004358-h2zzgf21\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: zjw5hvm9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_num_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_type: gru\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_num_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.8\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Aditya Patil\\Desktop\\IITM\\CS6910\\Assignment3\\wandb\\run-20230508_020132-zjw5hvm9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/zjw5hvm9' target=\"_blank\">hearty-sweep-2</a></strong> to <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/zjw5hvm9' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/zjw5hvm9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  1.3288587076082001 Training Accuracy:  0.00060546875 Validation Accuracy:  0.155029296875\n",
      "Epoch:  2  Loss:  0.8087141232874554 Training Accuracy:  0.00265625 Validation Accuracy:  0.191162109375\n",
      "Epoch:  3  Loss:  0.7323701176159723 Training Accuracy:  0.00349609375 Validation Accuracy:  0.226318359375\n",
      "Epoch:  4  Loss:  0.6862099949536781 Training Accuracy:  0.00478515625 Validation Accuracy:  0.22900390625\n",
      "Epoch:  5  Loss:  0.6572542751939342 Training Accuracy:  0.00513671875 Validation Accuracy:  0.232177734375\n",
      "Epoch:  6  Loss:  0.6369004509052203 Training Accuracy:  0.00560546875 Validation Accuracy:  0.261474609375\n",
      "Epoch:  7  Loss:  0.6157081455021631 Training Accuracy:  0.00587890625 Validation Accuracy:  0.26318359375\n",
      "Epoch:  8  Loss:  0.6028049119853565 Training Accuracy:  0.00578125 Validation Accuracy:  0.256591796875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▄▅▆▇▇▇█</td></tr><tr><td>Validation Accuracy</td><td>▁▃▆▆▆███</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>train_loss</td><td>▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>0.25859</td></tr><tr><td>Validation Accuracy</td><td>0.25659</td></tr><tr><td>epoch</td><td>8</td></tr><tr><td>train_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hearty-sweep-2</strong> at: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/zjw5hvm9' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/zjw5hvm9</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230508_020132-zjw5hvm9\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r06n1zj5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_num_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_num_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.8\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Aditya Patil\\Desktop\\IITM\\CS6910\\Assignment3\\wandb\\run-20230508_054133-r06n1zj5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/r06n1zj5' target=\"_blank\">radiant-sweep-3</a></strong> to <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/r06n1zj5' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/r06n1zj5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  1.8101083006384227 Training Accuracy:  1.953125e-05 Validation Accuracy:  0.027099609375\n",
      "Epoch:  2  Loss:  1.0464752934614605 Training Accuracy:  0.0002734375 Validation Accuracy:  0.17724609375\n",
      "Epoch:  3  Loss:  0.7637933519808877 Training Accuracy:  0.0009375 Validation Accuracy:  0.255126953125\n",
      "Epoch:  4  Loss:  0.6671597190584951 Training Accuracy:  0.0015234375 Validation Accuracy:  0.278076171875\n",
      "Epoch:  5  Loss:  0.6093918124759197 Training Accuracy:  0.00232421875 Validation Accuracy:  0.290283203125\n",
      "Epoch:  6  Loss:  0.5767493589543569 Training Accuracy:  0.00298828125 Validation Accuracy:  0.326416015625\n",
      "Epoch:  7  Loss:  0.5532929495242904 Training Accuracy:  0.0030859375 Validation Accuracy:  0.322998046875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▄▅▆▇██</td></tr><tr><td>Validation Accuracy</td><td>▁▅▆▇▇██</td></tr><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_loss</td><td>▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>0.40746</td></tr><tr><td>Validation Accuracy</td><td>0.323</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>train_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">radiant-sweep-3</strong> at: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/r06n1zj5' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/r06n1zj5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230508_054133-r06n1zj5\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: itts46x1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_num_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_num_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_type: gru\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.9\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Aditya Patil\\Desktop\\IITM\\CS6910\\Assignment3\\wandb\\run-20230508_085400-itts46x1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/itts46x1' target=\"_blank\">devoted-sweep-4</a></strong> to <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/itts46x1' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/itts46x1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  1.3618905724694463 Training Accuracy:  0.0001953125 Validation Accuracy:  0.09619140625\n",
      "Epoch:  2  Loss:  0.8016854731881666 Training Accuracy:  0.00037109375 Validation Accuracy:  0.166259765625\n",
      "Epoch:  3  Loss:  0.6841279535719589 Training Accuracy:  0.000703125 Validation Accuracy:  0.19091796875\n",
      "Epoch:  4  Loss:  0.6210895787675398 Training Accuracy:  0.00099609375 Validation Accuracy:  0.201171875\n",
      "Epoch:  5  Loss:  0.5802296510934286 Training Accuracy:  0.00099609375 Validation Accuracy:  0.226806640625\n",
      "Epoch:  6  Loss:  0.5514630736018035 Training Accuracy:  0.0011328125 Validation Accuracy:  0.22802734375\n",
      "Epoch:  7  Loss:  0.5344391596257769 Training Accuracy:  0.00189453125 Validation Accuracy:  0.258544921875\n",
      "Epoch:  8  Loss:  0.5130730210775606 Training Accuracy:  0.00142578125 Validation Accuracy:  0.254638671875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▃▄▅▆▇██</td></tr><tr><td>Validation Accuracy</td><td>▁▄▅▆▇▇██</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>train_loss</td><td>▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>0.28012</td></tr><tr><td>Validation Accuracy</td><td>0.25464</td></tr><tr><td>epoch</td><td>8</td></tr><tr><td>train_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">devoted-sweep-4</strong> at: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/itts46x1' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/itts46x1</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230508_085400-itts46x1\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: s6w9ukgr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_num_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_type: gru\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_num_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Aditya Patil\\Desktop\\IITM\\CS6910\\Assignment3\\wandb\\run-20230508_115554-s6w9ukgr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/s6w9ukgr' target=\"_blank\">classic-sweep-5</a></strong> to <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/7zf323te</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/s6w9ukgr' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/s6w9ukgr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  1.7555682921394429 Training Accuracy:  0.0020703125 Validation Accuracy:  0.112060546875\n",
      "Epoch:  2  Loss:  0.9947525073320662 Training Accuracy:  0.014609375 Validation Accuracy:  0.19775390625\n",
      "Epoch:  3  Loss:  0.7960267007086022 Training Accuracy:  0.03248046875 Validation Accuracy:  0.25146484375\n",
      "Epoch:  4  Loss:  0.6897453111264444 Training Accuracy:  0.04986328125 Validation Accuracy:  0.3017578125\n",
      "Epoch:  5  Loss:  0.6145203656592075 Training Accuracy:  0.06716796875 Validation Accuracy:  0.3271484375\n",
      "Epoch:  6  Loss:  0.5588792117509025 Training Accuracy:  0.0857421875 Validation Accuracy:  0.339599609375\n",
      "Epoch:  7  Loss:  0.5144277986840994 Training Accuracy:  0.10482421875 Validation Accuracy:  0.355224609375\n",
      "Epoch:  8  Loss:  0.4734701838295379 Training Accuracy:  0.12125 Validation Accuracy:  0.357177734375\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random', #grid, random\n",
    "    'metric': {\n",
    "        'name': 'Validation Accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedding_size': {\n",
    "            'values': [64, 128]   }, \n",
    "        'hidden_size': {\n",
    "            'values': [128, 256, 384, 512]   },\n",
    "        'encoder_num_layers': { \n",
    "            'values': [1, 2, 3]   },\n",
    "        'encoder_type': {'values' : ['gru', 'lstm']},\n",
    "        'decoder_num_layers': {\n",
    "            'values': [1, 2, 3]   },\n",
    "        'decoder_type': {'values' : ['gru' ,'lstm']},\n",
    "        'learning_rate': {'values': [0.0001, 0.0006, 0.001, 0.0003]},\n",
    "        'teacher_forcing_ratio': {'values': [0.5, 0.6, 0.7, 0.8, 0.9]},\n",
    "        'optimizer' : {'values': ['adam', 'sgd']},\n",
    "        'epochs' : { 'values' : [10]}\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_Assignment3\")\n",
    "\n",
    "def train():\n",
    "    config_defaults = {\n",
    "        'hidden_size': 256,\n",
    "        'encoder_num_layers': 1,\n",
    "        'encoder_type': 'gru',\n",
    "        'decoder_num_layers': 1,\n",
    "        'decoder_type': 'gru',\n",
    "        'learning_rate': 0.0001,\n",
    "        'teacher_forcing_ratio': 0.5,\n",
    "        'optimizer' : 'adam',\n",
    "        'epochs' : 5\n",
    "    }\n",
    "\n",
    "    run = wandb.init()\n",
    "    config = wandb.config\n",
    "    name_str = f\"nle_{wandb.config['encoder_num_layers']}_nld_{wandb.config['decoder_num_layers']}_lr_{wandb.config['learning_rate']}_eu_{wandb.config['encoder_type']}_du_{wandb.config['decoder_type']}\"\n",
    "    wandb.run.name = name_str\n",
    "    model = Transliterator(\n",
    "        encoder_hp={\n",
    "            'input_size': input_lang.n_letters,\n",
    "            'embedding_size' : config.embedding_size, \n",
    "            'hidden_size': config.hidden_size, \n",
    "            'num_layers': config.encoder_num_layers, \n",
    "            'type': config.encoder_type}, \n",
    "        decoder_hp={\n",
    "            'hidden_size': config.hidden_size, \n",
    "            'embedding_size': config.embedding_size,\n",
    "            'output_size': output_lang.n_letters, \n",
    "            'num_layers': config.decoder_num_layers, \n",
    "            'type': config.decoder_type}, \n",
    "        attn = False)\n",
    "    model.fit(train_pairs, validation_pairs, input_lang, output_lang, n_epochs = config.epochs, learning_rate = config.learning_rate, teacher_forcing_ratio = config.teacher_forcing_ratio, use_wanb = True)\n",
    "    run.finish()\n",
    "\n",
    "wandb.agent(sweep_id, train, count=5, project=\"CS6910_Assignment3\", entity=\"cs20b004\")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
