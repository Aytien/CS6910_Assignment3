{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import wandb \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs20b004\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 40\n",
    "WANDB_NOTEBOOK_NAME = 'Assignment3'\n",
    "WANDB_PROJECT_NAME = 'CS6910_A3'\n",
    "WANDB_ENTITY = 'cs20b004'\n",
    "wandb.login()\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        # self.word2index = {}\n",
    "        # self.word2count = {}\n",
    "        # self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        # self.n_words = 2  # Count SOS and EOS\n",
    "        self.letter_index = {}\n",
    "        self.letter_count = {}\n",
    "        self.index_letter = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_letters = 2  # Count SOS and EOS\n",
    "\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # for word in sentence.split(' '):\n",
    "        #     self.addWord(word)\n",
    "        for letter in word:\n",
    "            self.add_letter(letter)\n",
    "\n",
    "    def add_letter(self, letter):\n",
    "        # if word not in self.word2index:\n",
    "        #     self.word2index[word] = self.n_words\n",
    "        #     self.word2count[word] = 1\n",
    "        #     self.index2word[self.n_words] = word\n",
    "        #     self.n_words += 1\n",
    "        # else:\n",
    "        #     self.word2count[word] += 1\n",
    "        if letter not in self.letter_index:\n",
    "            self.letter_index[letter] = self.n_letters\n",
    "            self.letter_count[letter] = 1\n",
    "            self.index_letter[self.n_letters] = letter\n",
    "            self.n_letters += 1\n",
    "        else:\n",
    "            self.letter_count[letter] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words(output_lang, type = 'train', reverse = False):\n",
    "    lines = open('aksharantar_sampled/%s/%s_%s.csv' % (output_lang,output_lang,type), encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[s for s in l.split(',')] for l in lines]\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "    else:\n",
    "        pairs = [list(p) for p in pairs]\n",
    "    return pairs\n",
    "\n",
    "def init_lang(lang1, lang2, type = 'train', reverse = False):\n",
    "    pairs = read_words(lang2, type, reverse)\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    char = 'a'\n",
    "    for i in range(26):\n",
    "        input_lang.add_letter(char)\n",
    "        char = chr(ord(char) + 1)\n",
    "    char = '\\u0900'\n",
    "    for i in range(128):\n",
    "        output_lang.add_letter(char)\n",
    "        char = chr(ord(char) + 1)\n",
    "    # for pair in pairs:\n",
    "    #     input_lang.add_word(pair[0])\n",
    "    #     output_lang.add_word(pair[1])\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_LENGTH:  40\n"
     ]
    }
   ],
   "source": [
    "pairs = read_words('mar',type = 'train')\n",
    "for p in pairs:\n",
    "    MAX_LENGTH = max(MAX_LENGTH, len(p[1]))\n",
    "print(\"MAX_LENGTH: \", MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def indexesFromSentence(lang, sentence):\n",
    "#     return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "def word_to_index(lang, word):\n",
    "    return [lang.letter_index[letter] for letter in word]\n",
    "\n",
    "\n",
    "# def tensorFromSentence(lang, sentence):\n",
    "#     indexes = indexesFromSentence(lang, sentence)\n",
    "#     indexes.append(EOS_token)\n",
    "#     return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def word_to_tensor(lang, word):\n",
    "    indexes = word_to_index(lang, word)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "# def tensorsFromPair(pair):\n",
    "#     input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "#     target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "#     return (input_tensor, target_tensor)\n",
    "\n",
    "def pair_to_tensor(input_lang,output_lang, pair):\n",
    "    input_tensor = word_to_tensor(input_lang, pair[0])\n",
    "    target_tensor = word_to_tensor(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, type = 'gru', nonlinearity = 'tanh', dropout_p = 0.1, bidirectional = False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.dropout_p = dropout_p\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.type = type\n",
    "        if self.type == 'gru':\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, num_layers = num_layers, bidirectional = self.bidirectional)\n",
    "        elif self.type == 'lstm':\n",
    "            self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers, bidirectional = self.bidirectional)\n",
    "        elif self.type == 'rnn':\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers = num_layers, bidirectional = self.bidirectional)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        if self.type == 'gru':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.type == 'lstm':\n",
    "            output, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
    "        elif self.type == 'rnn':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "        return output, hidden, cell\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers , 1, self.hidden_size, device=device)\n",
    "    \n",
    "    def init_cell(self):\n",
    "        return self.initHidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, output_size, num_layers, nonlinearity = 'tanh', dropout_p = 0.1, type = 'gru', bidirectional = False):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        # self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.type = type\n",
    "        if self.type == 'gru':\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, num_layers = num_layers, bidirectional = self.bidirectional)\n",
    "        elif self.type == 'lstm':\n",
    "            self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers, bidirectional = self.bidirectional)\n",
    "        elif self.type == 'rnn':\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers = num_layers, bidirectional = self.bidirectional)\n",
    "            \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = self.dropout(output)\n",
    "        output = F.relu(output)\n",
    "        if self.type == 'gru':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.type == 'lstm':\n",
    "            output, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
    "        elif self.type == 'rnn':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "        \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, cell\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "    \n",
    "        \n",
    "    def init_cell(self):\n",
    "        return self.initHidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, output_size, num_layers, nonlinearity = 'tanh', dropout_p=0.1 , max_length=MAX_LENGTH, type = 'gru', bidirectional = False):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(self.output_size, self.embedding_size)\n",
    "        self.attn = nn.Linear(self.hidden_size + self.embedding_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size + self.embedding_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.bidirectional = bidirectional\n",
    "        self.type = type\n",
    "        \n",
    "        if self.type == 'gru':\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers = num_layers, bidirectional = self.bidirectional)\n",
    "        elif self.type == 'lstm':\n",
    "            self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers = num_layers, bidirectional = self.bidirectional)\n",
    "        elif self.type == 'rnn':\n",
    "            self.rnn = nn.RNN(hidden_size, hidden_size, num_layers = num_layers, bidirectional = self.bidirectional)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        if self.type == 'gru':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.type == 'lstm':\n",
    "            output, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
    "        elif self.type == 'rnn':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, cell, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "        \n",
    "    def init_cell(self):\n",
    "        return self.initHidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transliterator():\n",
    "    def __init__(self, encoder_hp, decoder_hp, attn = True):\n",
    "        self.encoder = EncoderRNN(\n",
    "                        encoder_hp['input_size'],\n",
    "                        encoder_hp['embedding_size'],\n",
    "                        encoder_hp['hidden_size'], \n",
    "                        encoder_hp['num_layers'], \n",
    "                        type = encoder_hp['type'],\n",
    "                        dropout_p= encoder_hp['dropout_p'],\n",
    "                        bidirectional = encoder_hp['bidirectional']\n",
    "                        ).to(device)\n",
    "        self.attn = attn\n",
    "        if attn:\n",
    "            self.decoder = AttnDecoderRNN(\n",
    "                            decoder_hp['hidden_size'], \n",
    "                            decoder_hp['embedding_size'],\n",
    "                            decoder_hp['output_size'], \n",
    "                            decoder_hp['num_layers'], \n",
    "                            type = decoder_hp['type'],\n",
    "                            dropout_p = decoder_hp['dropout_p'],\n",
    "                            bidirectional = decoder_hp['bidirectional']\n",
    "                            ).to(device)\n",
    "        else:\n",
    "            self.decoder = DecoderRNN(\n",
    "                            decoder_hp['hidden_size'], \n",
    "                            decoder_hp['embedding_size'],\n",
    "                            decoder_hp['output_size'], \n",
    "                            decoder_hp['num_layers'],  \n",
    "                            type = decoder_hp['type'],\n",
    "                            dropout_p = decoder_hp['dropout_p'],\n",
    "                            bidirectional = decoder_hp['bidirectional']\n",
    "                            ).to(device)\n",
    "\n",
    "    def train(self, input_tensor, target_tensor, max_length=MAX_LENGTH):\n",
    "        encoder_hidden = self.encoder.initHidden()\n",
    "        encoder_cell = self.encoder.init_cell()\n",
    "\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, self.encoder.hidden_size, device=device)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden, encoder_cell = self.encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)\n",
    "        decoder_cell = encoder_cell[encoder_cell.shape[0] - 1].unsqueeze(0)\n",
    "\n",
    "        decoded_word = ''\n",
    "        for i in range(self.decoder.num_layers - 1):\n",
    "            decoder_hidden = torch.cat((decoder_hidden, encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)), 0)\n",
    "            decoder_cell = torch.cat((decoder_cell, encoder_cell[encoder_cell.shape[0] - 1].unsqueeze(0)), 0)\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n",
    "        # print(encoder_outputs.shape)\n",
    "        if use_teacher_forcing:\n",
    "            for di in range(target_length):\n",
    "                if self.attn:\n",
    "                    decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  \n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "                decoded_word += self.output_lang.index_letter[topi.item()]\n",
    "                decoder_input = target_tensor[di]\n",
    "        else:\n",
    "            for di in range(target_length):\n",
    "                if self.attn:\n",
    "                    decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_cell, decoder_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  \n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "                decoded_word += self.output_lang.index_letter[topi.item()]\n",
    "                \n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        \n",
    "        return loss.item() / target_length, decoded_word \n",
    "    \n",
    "    def fit( self, train_io_pair, val_io_pair, input_lang, output_lang, n_epochs,optimizer = 'adam', criterion = nn.NLLLoss(), print_every=1000, learning_rate=0.0001, teacher_forcing_ratio = 0.5, use_wanb = False):\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "\n",
    "        if(optimizer == 'adam'):\n",
    "            self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=learning_rate)\n",
    "            self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=learning_rate)\n",
    "        elif(optimizer == 'sgd'):\n",
    "            self.encoder_optimizer = optim.SGD(self.encoder.parameters(), lr=learning_rate)\n",
    "            self.decoder_optimizer = optim.SGD(self.decoder.parameters(), lr=learning_rate)\n",
    "            \n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.n_epochs = n_epochs\n",
    "        self.print_every = print_every\n",
    "        \n",
    "        self.all_losses = []\n",
    "        self.total_loss = 0\n",
    "\n",
    "        # for i in range(len(train_io_pair)):\n",
    "        #     train_io_pair[i] = pair_to_tensor(train_io_pair[i])\n",
    "        training_pairs = [pair_to_tensor(self.input_lang, self.output_lang, train_io_pair[i]) for i in range(len(train_io_pair))]\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        for epc in range(1,n_epochs + 1):\n",
    "            correct = 0\n",
    "            for inp in range(len(train_io_pair)):\n",
    "                input_tensor = training_pairs[inp][0]\n",
    "                target_tensor = training_pairs[inp][1]\n",
    "                loss, decoder_output = self.train(input_tensor, target_tensor)\n",
    "                self.total_loss += loss\n",
    "                self.all_losses.append(loss)\n",
    "                if(decoder_output == train_io_pair[inp][1]):\n",
    "                    correct += 1\n",
    "            train_acc = self.eval(train_io_pair)\n",
    "            train_accs.append(train_acc)\n",
    "            val_acc = self.eval(val_io_pair)\n",
    "            val_accs.append(val_acc)\n",
    "            print(\"Epoch: \", epc, \" Loss: \", self.total_loss / len(train_io_pair),\n",
    "                  \"Training Accuracy: \", train_acc,\n",
    "                  \"Validation Accuracy: \", val_acc)\n",
    "            self.total_loss = 0\n",
    "            if(use_wanb):\n",
    "                wandb.log({\n",
    "                    \"epoch\" : epc,\n",
    "                    \"train_loss\" : self.total_loss / len(train_io_pair),\n",
    "                    \"Training Accuracy\": train_acc, \n",
    "                    \"Validation Accuracy\": val_acc\n",
    "                    })\n",
    "            \n",
    "            if len(val_accs) > 2 and val_accs[-1] < val_accs[-2]:\n",
    "                break\n",
    "            if len(train_accs) > 2 and train_accs[-1] < train_accs[-2]:\n",
    "                break\n",
    "        \n",
    "        torch.save(self.encoder.state_dict(), f'Saved_models/val_acc_{int(val_accs[-1]*100)}encoder.pth')\n",
    "        torch.save(self.decoder.state_dict(), f'Saved_models/val_acc_{int(val_accs[-1]*100)}decoder.pth')\n",
    "\n",
    "    def eval(self, io_pairs):\n",
    "        correct = 0\n",
    "        for i in range(len(io_pairs)):\n",
    "            output, _ = self.predict(io_pairs[i][0])\n",
    "            if output == io_pairs[i][1]:\n",
    "                correct += 1\n",
    "        return correct / len(io_pairs)\n",
    "    \n",
    "    def predict(self, input_word, max_length=MAX_LENGTH):\n",
    "        with torch.no_grad():\n",
    "            input_tensor = word_to_tensor(self.input_lang, input_word)\n",
    "            input_length = input_tensor.size()[0]\n",
    "            encoder_hidden = self.encoder.initHidden()\n",
    "            encoder_cell = self.encoder.init_cell()\n",
    "            encoder_outputs = torch.zeros(max_length, self.encoder.hidden_size, device=device)\n",
    "\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden, encoder_cell = self.encoder(input_tensor[ei],\n",
    "                                                        encoder_hidden, encoder_cell)\n",
    "                encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "            decoder_hidden = encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)\n",
    "            decoder_cell = encoder_cell[encoder_cell.shape[0] - 1].unsqueeze(0)\n",
    "\n",
    "            decoded_word = ''\n",
    "            for i in range(self.decoder.num_layers - 1):\n",
    "                decoder_hidden = torch.cat((decoder_hidden, encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)), 0)\n",
    "                decoder_cell = torch.cat((decoder_cell, encoder_cell[encoder_cell.shape[0] - 1].unsqueeze(0)), 0)\n",
    "\n",
    "            decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "            for di in range(max_length):\n",
    "                if self.attn:\n",
    "                    decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "                    decoder_attentions[di] = decoder_attention.data\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden, decoder_cell = self.decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell)\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                if topi.item() == EOS_token:\n",
    "                    # decoded_words.append('<EOS>')\n",
    "                    # decoded_word += '<EOS>'\n",
    "                    break\n",
    "                else:\n",
    "                    # decoded_words.append(output_lang.letter_index[topi.item()])\n",
    "                    decoded_word += self.output_lang.index_letter[topi.item()]\n",
    "                    # print(output_lang.index_letter[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            return decoded_word, decoder_attentions[:di + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  4.624278964924416 Training Accuracy:  0.0 Validation Accuracy:  0.0\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, train_pairs = init_lang( 'eng','mar', type = 'train', reverse = False)\n",
    "validation_pairs = read_words('mar', type = 'valid')\n",
    "# print(input_lang.n_letters)\n",
    "encoder_hp = {\n",
    "    'input_size': input_lang.n_letters, \n",
    "    'embedding_size': 64, \n",
    "    'hidden_size': 512, \n",
    "    'num_layers': 2, \n",
    "    'dropout_p': 0.1,\n",
    "    'type': 'gru',\n",
    "    'bidirectional': False}\n",
    "decoder_hp = {\n",
    "    'hidden_size': 512, \n",
    "    'embedding_size': 64, \n",
    "    'output_size': output_lang.n_letters, \n",
    "    'num_layers': 2, \n",
    "    'dropout_p': 0.1,\n",
    "    'type': 'lstm',\n",
    "    'bidirectional': False}\n",
    "\n",
    "model = Transliterator(encoder_hp=encoder_hp, decoder_hp=decoder_hp, attn = True)\n",
    "\n",
    "model.fit(train_pairs[:10], validation_pairs, input_lang, output_lang, optimizer='adam', n_epochs = 1, learning_rate = 0.0003, teacher_forcing_ratio = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 16elfeq4\n",
      "Sweep URL: https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gyjjbz0b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_num_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_p: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_num_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.6\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Aditya Patil\\Desktop\\IITM\\CS6910\\Assignment3\\wandb\\run-20230508_180431-gyjjbz0b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/gyjjbz0b' target=\"_blank\">feasible-sweep-1</a></strong> to <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/gyjjbz0b' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/gyjjbz0b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  1.381055039081946 Training Accuracy:  0.16001953125 Validation Accuracy:  0.185302734375\n",
      "Epoch:  2  Loss:  0.8919842423450187 Training Accuracy:  0.22787109375 Validation Accuracy:  0.2431640625\n",
      "Epoch:  3  Loss:  0.8007720594848559 Training Accuracy:  0.25869140625 Validation Accuracy:  0.262451171875\n",
      "Epoch:  4  Loss:  0.7622321201566129 Training Accuracy:  0.2811328125 Validation Accuracy:  0.263671875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5  Loss:  0.7372006558409755 Training Accuracy:  0.30046875 Validation Accuracy:  0.27587890625\n",
      "Epoch:  6  Loss:  0.7198768593926317 Training Accuracy:  0.3105859375 Validation Accuracy:  0.28955078125\n",
      "Epoch:  7  Loss:  0.7090840508862121 Training Accuracy:  0.31076171875 Validation Accuracy:  0.298583984375\n",
      "Epoch:  8  Loss:  0.6954868918602546 Training Accuracy:  0.32615234375 Validation Accuracy:  0.298095703125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▄▅▆▇▇▇█</td></tr><tr><td>Validation Accuracy</td><td>▁▅▆▆▇▇██</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>train_loss</td><td>▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>0.32615</td></tr><tr><td>Validation Accuracy</td><td>0.2981</td></tr><tr><td>epoch</td><td>8</td></tr><tr><td>train_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">feasible-sweep-1</strong> at: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/gyjjbz0b' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/gyjjbz0b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230508_180431-gyjjbz0b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xqceb60b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_num_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_p: 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_num_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.8\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Aditya Patil\\Desktop\\IITM\\CS6910\\Assignment3\\wandb\\run-20230508_213759-xqceb60b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/xqceb60b' target=\"_blank\">cool-sweep-2</a></strong> to <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/xqceb60b' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/xqceb60b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  1.466882996346174 Training Accuracy:  0.1180078125 Validation Accuracy:  0.130615234375\n",
      "Epoch:  2  Loss:  0.7893773288283512 Training Accuracy:  0.20400390625 Validation Accuracy:  0.21533203125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3  Loss:  0.6716544884840387 Training Accuracy:  0.249453125 Validation Accuracy:  0.237060546875\n",
      "Epoch:  4  Loss:  0.6184647568022013 Training Accuracy:  0.28041015625 Validation Accuracy:  0.272216796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5  Loss:  0.5871179984695075 Training Accuracy:  0.29119140625 Validation Accuracy:  0.27490234375\n",
      "Epoch:  6  Loss:  0.5613635891281958 Training Accuracy:  0.31212890625 Validation Accuracy:  0.288330078125\n",
      "Epoch:  7  Loss:  0.5474868726029418 Training Accuracy:  0.3283984375 Validation Accuracy:  0.29248046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8  Loss:  0.5318176805024158 Training Accuracy:  0.339765625 Validation Accuracy:  0.30712890625\n",
      "Epoch:  9  Loss:  0.5186064445796796 Training Accuracy:  0.35107421875 Validation Accuracy:  0.2978515625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▄▅▆▆▇▇██</td></tr><tr><td>Validation Accuracy</td><td>▁▄▅▇▇▇▇██</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>train_loss</td><td>▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>0.35107</td></tr><tr><td>Validation Accuracy</td><td>0.29785</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>train_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cool-sweep-2</strong> at: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/xqceb60b' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/xqceb60b</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230508_213759-xqceb60b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u8uo7xgf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_num_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_p: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_num_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.8\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Aditya Patil\\Desktop\\IITM\\CS6910\\Assignment3\\wandb\\run-20230509_022611-u8uo7xgf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/u8uo7xgf' target=\"_blank\">fine-sweep-3</a></strong> to <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/u8uo7xgf' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/u8uo7xgf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  1.1570751693906092 Training Accuracy:  0.29515625 Validation Accuracy:  0.298095703125\n",
      "Epoch:  2  Loss:  0.5130323885487139 Training Accuracy:  0.44453125 Validation Accuracy:  0.37939453125\n",
      "Epoch:  3  Loss:  0.4115000028591645 Training Accuracy:  0.536875 Validation Accuracy:  0.4052734375\n",
      "Epoch:  4  Loss:  0.35609461650369845 Training Accuracy:  0.59189453125 Validation Accuracy:  0.42138671875\n",
      "Epoch:  5  Loss:  0.3180255054992619 Training Accuracy:  0.6458984375 Validation Accuracy:  0.4296875\n",
      "Epoch:  6  Loss:  0.28743820353279553 Training Accuracy:  0.68470703125 Validation Accuracy:  0.44775390625\n",
      "Epoch:  7  Loss:  0.2677807558610049 Training Accuracy:  0.7144921875 Validation Accuracy:  0.445556640625\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▃▅▆▇██</td></tr><tr><td>Validation Accuracy</td><td>▁▅▆▇▇██</td></tr><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_loss</td><td>▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>0.71449</td></tr><tr><td>Validation Accuracy</td><td>0.44556</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>train_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fine-sweep-3</strong> at: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/u8uo7xgf' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/u8uo7xgf</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_022611-u8uo7xgf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: maatnb4t with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_num_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_p: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_num_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.8\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Aditya Patil\\Desktop\\IITM\\CS6910\\Assignment3\\wandb\\run-20230509_053211-maatnb4t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/maatnb4t' target=\"_blank\">fancy-sweep-4</a></strong> to <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/maatnb4t' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/maatnb4t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  1.1896185391693734 Training Accuracy:  0.16169921875 Validation Accuracy:  0.19140625\n",
      "Epoch:  2  Loss:  0.733583818485473 Training Accuracy:  0.22208984375 Validation Accuracy:  0.2392578125\n",
      "Epoch:  3  Loss:  0.6612584183122344 Training Accuracy:  0.24599609375 Validation Accuracy:  0.248779296875\n",
      "Epoch:  4  Loss:  0.6360235863341287 Training Accuracy:  0.26388671875 Validation Accuracy:  0.257080078125\n",
      "Epoch:  5  Loss:  0.6127345598996519 Training Accuracy:  0.27818359375 Validation Accuracy:  0.26513671875\n",
      "Epoch:  6  Loss:  0.6021987871694665 Training Accuracy:  0.28728515625 Validation Accuracy:  0.279541015625\n",
      "Epoch:  7  Loss:  0.5980350768390703 Training Accuracy:  0.2891796875 Validation Accuracy:  0.2783203125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▄▆▇▇██</td></tr><tr><td>Validation Accuracy</td><td>▁▅▆▆▇██</td></tr><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_loss</td><td>▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>0.28918</td></tr><tr><td>Validation Accuracy</td><td>0.27832</td></tr><tr><td>epoch</td><td>7</td></tr><tr><td>train_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fancy-sweep-4</strong> at: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/maatnb4t' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/maatnb4t</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_053211-maatnb4t\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0sesbkhg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_num_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_p: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_num_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.8\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Aditya Patil\\Desktop\\IITM\\CS6910\\Assignment3\\wandb\\run-20230509_090840-0sesbkhg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/0sesbkhg' target=\"_blank\">golden-sweep-5</a></strong> to <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/16elfeq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/0sesbkhg' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/0sesbkhg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  1.2263363691413884 Training Accuracy:  0.192265625 Validation Accuracy:  0.210693359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectTimeout), entering retry loop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2  Loss:  0.6619147873380858 Training Accuracy:  0.26787109375 Validation Accuracy:  0.27001953125\n",
      "Epoch:  3  Loss:  0.5753837522447678 Training Accuracy:  0.3173046875 Validation Accuracy:  0.2998046875\n",
      "Epoch:  4  Loss:  0.5344100113667334 Training Accuracy:  0.35189453125 Validation Accuracy:  0.322509765625\n",
      "Epoch:  5  Loss:  0.5080324761563748 Training Accuracy:  0.36953125 Validation Accuracy:  0.33154296875\n",
      "Epoch:  6  Loss:  0.4863904313547555 Training Accuracy:  0.38876953125 Validation Accuracy:  0.337646484375\n",
      "Epoch:  7  Loss:  0.4780769316244548 Training Accuracy:  0.39533203125 Validation Accuracy:  0.33984375\n",
      "Epoch:  8  Loss:  0.4663358917371159 Training Accuracy:  0.40912109375 Validation Accuracy:  0.335693359375\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>▁▃▅▆▇▇██</td></tr><tr><td>Validation Accuracy</td><td>▁▄▆▇████</td></tr><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>train_loss</td><td>▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Accuracy</td><td>0.40912</td></tr><tr><td>Validation Accuracy</td><td>0.33569</td></tr><tr><td>epoch</td><td>8</td></tr><tr><td>train_loss</td><td>0.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">golden-sweep-5</strong> at: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/0sesbkhg' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/0sesbkhg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230509_090840-0sesbkhg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'name' : 'Dropout_in_LSTM',\n",
    "    'method': 'random', #grid, random\n",
    "    'metric': {\n",
    "        'name': 'Validation Accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedding_size': {\n",
    "            'values': [64, 128]   }, \n",
    "        'hidden_size': {\n",
    "            'values': [ 384, 512]   },\n",
    "        'encoder_num_layers': { \n",
    "            'values': [2, 3]   },\n",
    "        'encoder_type': {'values' : ['lstm']},\n",
    "        'decoder_num_layers': {\n",
    "            'values': [2, 3]   },\n",
    "        'decoder_type': {'values' : ['lstm']},\n",
    "        'learning_rate': {'values': [0.0006, 0.001, 0.0003]},\n",
    "        'teacher_forcing_ratio': {'values': [ 0.6, 0.7, 0.8,]},\n",
    "        'optimizer' : {'values': ['adam']},\n",
    "        'epochs' : { 'values' : [10]},\n",
    "        'dropout_p' : {'values' : [0.0]},\n",
    "        'bidirectional' : {'values' : [False]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_Assignment3\")\n",
    "\n",
    "def train():\n",
    "    config_defaults = {\n",
    "        'hidden_size': 256,\n",
    "        'encoder_num_layers': 1,\n",
    "        'encoder_type': 'gru',\n",
    "        'decoder_num_layers': 1,\n",
    "        'decoder_type': 'gru',\n",
    "        'learning_rate': 0.0001,\n",
    "        'teacher_forcing_ratio': 0.5,\n",
    "        'optimizer' : 'adam',\n",
    "        'epochs' : 5,\n",
    "        'dropout_p' : 0.1,\n",
    "        'bidirectional' : False\n",
    "    }\n",
    "\n",
    "    run = wandb.init()\n",
    "    config = wandb.config\n",
    "    name_str = f\"nle_{wandb.config['encoder_num_layers']}_nld_{wandb.config['decoder_num_layers']}_lr_{wandb.config['learning_rate']}_eu_{wandb.config['encoder_type']}_du_{wandb.config['decoder_type']}\"\n",
    "    wandb.run.name = name_str\n",
    "    model = Transliterator(\n",
    "        encoder_hp={\n",
    "            'input_size': input_lang.n_letters,\n",
    "            'embedding_size' : config.embedding_size, \n",
    "            'hidden_size': config.hidden_size, \n",
    "            'num_layers': config.encoder_num_layers, \n",
    "            'type': config.encoder_type,\n",
    "            'dropout_p' : config.dropout_p,\n",
    "            'bidirectional' : config.bidirectional\n",
    "            }, \n",
    "        decoder_hp={\n",
    "            'hidden_size': config.hidden_size, \n",
    "            'embedding_size': config.embedding_size,\n",
    "            'output_size': output_lang.n_letters, \n",
    "            'num_layers': config.decoder_num_layers, \n",
    "            'type': config.decoder_type,\n",
    "            'dropout_p' : config.dropout_p,\n",
    "            'bidirectional' : config.bidirectional\n",
    "            }, \n",
    "        attn = False)\n",
    "    model.fit(train_pairs, validation_pairs, input_lang, output_lang, n_epochs = config.epochs, learning_rate = config.learning_rate, teacher_forcing_ratio = config.teacher_forcing_ratio, use_wanb = True)\n",
    "    run.finish()\n",
    "\n",
    "wandb.agent(sweep_id, train, count=4, project=\"CS6910_Assignment3\", entity=\"cs20b004\")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hp = {\n",
    "    'input_size': input_lang.n_letters,\n",
    "    'embedding_size' : 64,\n",
    "    'hidden_size': 384,\n",
    "    'num_layers': 1,\n",
    "    'type': 'lstm'}\n",
    "decoder_hp = {\n",
    "    'hidden_size': 384,\n",
    "    'embedding_size': 64,\n",
    "    'output_size': output_lang.n_letters,\n",
    "    'num_layers': 1,\n",
    "    'type': 'gru'}\n",
    "model = Transliterator(encoder_hp=encoder_hp, decoder_hp=decoder_hp, attn = False)\n",
    "model.encoder.load_state_dict(torch.load('Saved_models/Best_encoder.pth'))\n",
    "model.decoder.load_state_dict(torch.load('Saved_models/Best_decoder.pth'))\n",
    "model.input_lang = input_lang\n",
    "model.output_lang = output_lang\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trans_sentence(sentence):\n",
    "    for i in range(len(sentence.split(' '))):\n",
    "        print(model.predict(sentence.split(' ')[i])[0], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \tbidirectional: False\n",
    " \tdecoder_num_layers: 2\n",
    " \tdecoder_type: lstm\n",
    " \tdropout_p: 0\n",
    " \tembedding_size: 64\n",
    " \tencoder_num_layers: 2\n",
    " \tencoder_type: lstm\n",
    " \tepochs: 10\n",
    " \thidden_size: 384\n",
    " \tlearning_rate: 0.0003\n",
    " \toptimizer: adam\n",
    " \tteacher_forcing_ratio: 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DecoderRNN:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([130, 64]) from checkpoint, the shape in current model is torch.Size([66, 64]).\n\tsize mismatch for out.weight: copying a param with shape torch.Size([130, 384]) from checkpoint, the shape in current model is torch.Size([66, 384]).\n\tsize mismatch for out.bias: copying a param with shape torch.Size([130]) from checkpoint, the shape in current model is torch.Size([66]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m best_m \u001b[39m=\u001b[39m Transliterator(encoder_hp\u001b[39m=\u001b[39mencoder_hp_1, decoder_hp\u001b[39m=\u001b[39mdecoder_hp_1, attn \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m best_m\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mSaved_models/val_acc_44encoder.pth\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m---> 21\u001b[0m best_m\u001b[39m.\u001b[39;49mdecoder\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mSaved_models/val_acc_44decoder.pth\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m     22\u001b[0m best_m\u001b[39m.\u001b[39minput_lang \u001b[39m=\u001b[39m input_lang\n\u001b[0;32m     23\u001b[0m best_m\u001b[39m.\u001b[39moutput_lang \u001b[39m=\u001b[39m output_lang\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for DecoderRNN:\n\tsize mismatch for embedding.weight: copying a param with shape torch.Size([130, 64]) from checkpoint, the shape in current model is torch.Size([66, 64]).\n\tsize mismatch for out.weight: copying a param with shape torch.Size([130, 384]) from checkpoint, the shape in current model is torch.Size([66, 384]).\n\tsize mismatch for out.bias: copying a param with shape torch.Size([130]) from checkpoint, the shape in current model is torch.Size([66])."
     ]
    }
   ],
   "source": [
    "encoder_hp_1= {\n",
    "    'input_size': input_lang.n_letters,\n",
    "    'embedding_size' : 64,\n",
    "    'hidden_size': 384,\n",
    "    'num_layers': 2,\n",
    "    'type': 'lstm',\n",
    "    'dropout_p' : 0,\n",
    "    'bidirectional' : False\n",
    "    }\n",
    "decoder_hp_1 = {\n",
    "    'hidden_size': 384,\n",
    "    'embedding_size': 64,\n",
    "    'output_size': output_lang.n_letters,\n",
    "    'num_layers': 2,\n",
    "    'dropout_p' : 0,\n",
    "    'bidirectional' : False,\n",
    "    'type': 'lstm'}\n",
    "\n",
    "best_m = Transliterator(encoder_hp=encoder_hp_1, decoder_hp=decoder_hp_1, attn = False)\n",
    "best_m.encoder.load_state_dict(torch.load('Saved_models/val_acc_44encoder.pth'))\n",
    "best_m.decoder.load_state_dict(torch.load('Saved_models/val_acc_44decoder.pth'))\n",
    "best_m.input_lang = input_lang\n",
    "best_m.output_lang = output_lang\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
