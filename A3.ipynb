{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import wandb \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs20b004\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 40\n",
    "WANDB_NOTEBOOK_NAME = 'Assignment3'\n",
    "WANDB_PROJECT_NAME = 'CS6910_A3'\n",
    "WANDB_ENTITY = 'cs20b004'\n",
    "wandb.login()\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        # self.word2index = {}\n",
    "        # self.word2count = {}\n",
    "        # self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        # self.n_words = 2  # Count SOS and EOS\n",
    "        self.letter_index = {}\n",
    "        self.letter_count = {}\n",
    "        self.index_letter = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_letters = 2  # Count SOS and EOS\n",
    "\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # for word in sentence.split(' '):\n",
    "        #     self.addWord(word)\n",
    "        for letter in word:\n",
    "            self.add_letter(letter)\n",
    "\n",
    "    def add_letter(self, letter):\n",
    "        # if word not in self.word2index:\n",
    "        #     self.word2index[word] = self.n_words\n",
    "        #     self.word2count[word] = 1\n",
    "        #     self.index2word[self.n_words] = word\n",
    "        #     self.n_words += 1\n",
    "        # else:\n",
    "        #     self.word2count[word] += 1\n",
    "        if letter not in self.letter_index:\n",
    "            self.letter_index[letter] = self.n_letters\n",
    "            self.letter_count[letter] = 1\n",
    "            self.index_letter[self.n_letters] = letter\n",
    "            self.n_letters += 1\n",
    "        else:\n",
    "            self.letter_count[letter] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words(output_lang, type = 'train', reverse = False):\n",
    "    lines = open('aksharantar_sampled/%s/%s_%s.csv' % (output_lang,output_lang,type), encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[s for s in l.split(',')] for l in lines]\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "    else:\n",
    "        pairs = [list(p) for p in pairs]\n",
    "    return pairs\n",
    "\n",
    "def init_lang(lang1, lang2, type = 'train', reverse = False):\n",
    "    pairs = read_words(lang2, type, reverse)\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    char = 'a'\n",
    "    for i in range(26):\n",
    "        input_lang.add_letter(char)\n",
    "        char = chr(ord(char) + 1)\n",
    "    char = '\\u0900'\n",
    "    for i in range(128):\n",
    "        output_lang.add_letter(char)\n",
    "        char = chr(ord(char) + 1)\n",
    "\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_LENGTH:  40\n"
     ]
    }
   ],
   "source": [
    "pairs = read_words('mar',type = 'train')\n",
    "for p in pairs:\n",
    "    MAX_LENGTH = max(MAX_LENGTH, len(p[1]))\n",
    "print(\"MAX_LENGTH: \", MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def indexesFromSentence(lang, sentence):\n",
    "#     return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "def word_to_index(lang, word):\n",
    "    return [lang.letter_index[letter] for letter in word]\n",
    "\n",
    "\n",
    "# def tensorFromSentence(lang, sentence):\n",
    "#     indexes = indexesFromSentence(lang, sentence)\n",
    "#     indexes.append(EOS_token)\n",
    "#     return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def word_to_tensor(lang, word):\n",
    "    indexes = word_to_index(lang, word)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "# def tensorsFromPair(pair):\n",
    "#     input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "#     target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "#     return (input_tensor, target_tensor)\n",
    "\n",
    "def pair_to_tensor(input_lang,output_lang, pair):\n",
    "    input_tensor = word_to_tensor(input_lang, pair[0])\n",
    "    target_tensor = word_to_tensor(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, type = 'gru', nonlinearity = 'tanh'):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.type = type\n",
    "        if self.type == 'gru':\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'lstm':\n",
    "            self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'rnn':\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers = num_layers)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        if self.type == 'gru':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.type == 'lstm':\n",
    "            output, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
    "        elif self.type == 'rnn':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "        return output, hidden, cell\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers , 1, self.hidden_size, device=device)\n",
    "    \n",
    "    def init_cell(self):\n",
    "        return self.initHidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, output_size, num_layers, nonlinearity = 'tanh', dropout_p = 0.1, type = 'gru'):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        # self.dropout_p = dropout_p\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        # self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.num_layers = num_layers\n",
    "        self.type = type\n",
    "        if self.type == 'gru':\n",
    "            self.gru = nn.GRU(embedding_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'lstm':\n",
    "            self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'rnn':\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers = num_layers)\n",
    "            \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # output = self.dropout(output)\n",
    "        output = F.relu(output)\n",
    "        if self.type == 'gru':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.type == 'lstm':\n",
    "            output, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
    "        elif self.type == 'rnn':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "        \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden, cell\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "    \n",
    "        \n",
    "    def init_cell(self):\n",
    "        return self.initHidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding_size, output_size, num_layers, nonlinearity = 'tanh', dropout_p=0.1, max_length=MAX_LENGTH, type = 'gru'):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(self.output_size, self.embedding_size)\n",
    "        self.attn = nn.Linear(self.hidden_size + self.embedding_size, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size + self.embedding_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.type = type\n",
    "        \n",
    "        if self.type == 'gru':\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'lstm':\n",
    "            self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'rnn':\n",
    "            self.rnn = nn.RNN(hidden_size, hidden_size, num_layers = num_layers)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        if self.type == 'gru':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.type == 'lstm':\n",
    "            output, (hidden, cell) = self.lstm(output, (hidden, cell))\n",
    "        elif self.type == 'rnn':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, cell, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)\n",
    "        \n",
    "    def init_cell(self):\n",
    "        return self.initHidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transliterator():\n",
    "    def __init__(self, encoder_hp, decoder_hp, attn = True):\n",
    "        self.encoder = EncoderRNN(\n",
    "                        encoder_hp['input_size'],\n",
    "                        encoder_hp['embedding_size'],\n",
    "                        encoder_hp['hidden_size'], \n",
    "                        encoder_hp['num_layers'], \n",
    "                        type = encoder_hp['type']).to(device)\n",
    "        self.attn = attn\n",
    "        if attn:\n",
    "            self.decoder = AttnDecoderRNN(\n",
    "                            decoder_hp['hidden_size'], \n",
    "                            decoder_hp['embedding_size'],\n",
    "                            decoder_hp['output_size'], \n",
    "                            decoder_hp['num_layers'], \n",
    "                            type = decoder_hp['type']).to(device)\n",
    "        else:\n",
    "            self.decoder = DecoderRNN(\n",
    "                            decoder_hp['hidden_size'], \n",
    "                            decoder_hp['embedding_size'],\n",
    "                            decoder_hp['output_size'], \n",
    "                            decoder_hp['num_layers'],  \n",
    "                            type = decoder_hp['type']).to(device)\n",
    "\n",
    "    def train(self, input_tensor, target_tensor, max_length=MAX_LENGTH):\n",
    "        encoder_hidden = self.encoder.initHidden()\n",
    "        encoder_cell = self.encoder.init_cell()\n",
    "\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, self.encoder.hidden_size, device=device)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden, encoder_cell = self.encoder(input_tensor[ei], encoder_hidden, encoder_cell)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)\n",
    "        decoder_cell = encoder_cell[encoder_cell.shape[0] - 1].unsqueeze(0)\n",
    "\n",
    "        decoded_word = ''\n",
    "        for i in range(self.decoder.num_layers - 1):\n",
    "            decoder_hidden = torch.cat((decoder_hidden, encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)), 0)\n",
    "            decoder_cell = torch.cat((decoder_cell, encoder_cell[encoder_cell.shape[0] - 1].unsqueeze(0)), 0)\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n",
    "        # print(encoder_outputs.shape)\n",
    "        if use_teacher_forcing:\n",
    "            for di in range(target_length):\n",
    "                if self.attn:\n",
    "                    decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_hidden, decoder_cell)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  \n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "                decoded_word += self.output_lang.index_letter[topi.item()]\n",
    "                decoder_input = target_tensor[di]\n",
    "        else:\n",
    "            for di in range(target_length):\n",
    "                if self.attn:\n",
    "                    decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, decoder_cell, decoder_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  \n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "                decoded_word += self.output_lang.index_letter[topi.item()]\n",
    "                \n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        \n",
    "        return loss.item() / target_length, decoded_word \n",
    "    \n",
    "    def fit( self, train_io_pair, val_io_pair, input_lang, output_lang, n_epochs,optimizer = 'adam', criterion = nn.NLLLoss(), print_every=1000, learning_rate=0.0001, teacher_forcing_ratio = 0.5, use_wanb = False):\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "\n",
    "        if(optimizer == 'adam'):\n",
    "            self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=learning_rate)\n",
    "            self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=learning_rate)\n",
    "        elif(optimizer == 'sgd'):\n",
    "            self.encoder_optimizer = optim.SGD(self.encoder.parameters(), lr=learning_rate)\n",
    "            self.decoder_optimizer = optim.SGD(self.decoder.parameters(), lr=learning_rate)\n",
    "            \n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.n_epochs = n_epochs\n",
    "        self.print_every = print_every\n",
    "        \n",
    "        self.all_losses = []\n",
    "        self.total_loss = 0\n",
    "\n",
    "        # for i in range(len(train_io_pair)):\n",
    "        #     train_io_pair[i] = pair_to_tensor(train_io_pair[i])\n",
    "        training_pairs = [pair_to_tensor(self.input_lang, self.output_lang, train_io_pair[i]) for i in range(len(train_io_pair))]\n",
    "        train_accs = []\n",
    "        val_accs = []\n",
    "        for epc in range(1,n_epochs + 1):\n",
    "            correct = 0\n",
    "            for inp in range(len(train_io_pair)):\n",
    "                input_tensor = training_pairs[inp][0]\n",
    "                target_tensor = training_pairs[inp][1]\n",
    "                loss, decoder_output = self.train(input_tensor, target_tensor)\n",
    "                self.total_loss += loss\n",
    "                self.all_losses.append(loss)\n",
    "                if(decoder_output == train_io_pair[inp][1]):\n",
    "                    correct += 1\n",
    "            train_acc = self.eval(train_io_pair)\n",
    "            train_accs.append(train_acc)\n",
    "            val_acc = self.eval(val_io_pair)\n",
    "            val_accs.append(val_acc)\n",
    "            print(\"Epoch: \", epc, \" Loss: \", self.total_loss / len(train_io_pair),\n",
    "                  \"Training Accuracy: \", correct/len(train_io_pair),\n",
    "                  \"Validation Accuracy: \", val_acc)\n",
    "            self.total_loss = 0\n",
    "            if(use_wanb):\n",
    "                wandb.log({\n",
    "                    \"epoch\" : epc,\n",
    "                    \"train_loss\" : self.total_loss / len(train_io_pair),\n",
    "                    \"Training Accuracy\": train_acc, \n",
    "                    \"Validation Accuracy\": val_acc\n",
    "                    })\n",
    "            if len(val_accs) > 2 and val_accs[-1] < val_accs[-2]:\n",
    "                break\n",
    "            if len(train_accs) > 2 and train_accs[-1] < train_accs[-2]:\n",
    "                break\n",
    "        \n",
    "        torch.save(self.encoder.state_dict(), f'Saved_models/val_acc:{val_accs[-1]}encoder.pth')\n",
    "        torch.save(self.decoder.state_dict(), f'Saved_models/val_acc:{val_accs[-1]}decoder.pth')\n",
    "\n",
    "    def eval(self, io_pairs):\n",
    "        correct = 0\n",
    "        for i in range(len(io_pairs)):\n",
    "            output, _ = self.predict(io_pairs[i][0])\n",
    "            if output == io_pairs[i][1]:\n",
    "                correct += 1\n",
    "        return correct / len(io_pairs)\n",
    "    \n",
    "    def predict(self, input_word, max_length=MAX_LENGTH):\n",
    "        with torch.no_grad():\n",
    "            input_tensor = word_to_tensor(self.input_lang, input_word)\n",
    "            input_length = input_tensor.size()[0]\n",
    "            encoder_hidden = self.encoder.initHidden()\n",
    "            encoder_cell = self.encoder.init_cell()\n",
    "            encoder_outputs = torch.zeros(max_length, self.encoder.hidden_size, device=device)\n",
    "\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden, encoder_cell = self.encoder(input_tensor[ei],\n",
    "                                                        encoder_hidden, encoder_cell)\n",
    "                encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "            decoder_hidden = encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)\n",
    "            decoder_cell = encoder_cell[encoder_cell.shape[0] - 1].unsqueeze(0)\n",
    "\n",
    "            decoded_word = ''\n",
    "            for i in range(self.decoder.num_layers - 1):\n",
    "                decoder_hidden = torch.cat((decoder_hidden, encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)), 0)\n",
    "                decoder_cell = torch.cat((decoder_cell, encoder_cell[encoder_cell.shape[0] - 1].unsqueeze(0)), 0)\n",
    "\n",
    "            decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "            for di in range(max_length):\n",
    "                if self.attn:\n",
    "                    decoder_output, decoder_hidden, decoder_cell, decoder_attention = self.decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell, encoder_outputs)\n",
    "                    decoder_attentions[di] = decoder_attention.data\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden, decoder_cell = self.decoder(\n",
    "                    decoder_input, decoder_hidden, decoder_cell)\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                if topi.item() == EOS_token:\n",
    "                    # decoded_words.append('<EOS>')\n",
    "                    # decoded_word += '<EOS>'\n",
    "                    break\n",
    "                else:\n",
    "                    # decoded_words.append(output_lang.letter_index[topi.item()])\n",
    "                    decoded_word += self.output_lang.index_letter[topi.item()]\n",
    "                    # print(output_lang.index_letter[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            return decoded_word, decoder_attentions[:di + 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  2.929136398452687 Training Accuracy:  0.0 Validation Accuracy:  0.000244140625\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, train_pairs = init_lang( 'eng','mar', type = 'train', reverse = False)\n",
    "validation_pairs = read_words('mar', type = 'valid')\n",
    "# print(input_lang.n_letters)\n",
    "encoder_hp = {\n",
    "    'input_size': input_lang.n_letters, \n",
    "    'embedding_size': 64, \n",
    "    'hidden_size': 512, \n",
    "    'num_layers': 1, \n",
    "    'type': 'gru'}\n",
    "decoder_hp = {\n",
    "    'hidden_size': 512, \n",
    "    'embedding_size': 64, \n",
    "    'output_size': output_lang.n_letters, \n",
    "    'num_layers': 1, \n",
    "    'type': 'lstm'}\n",
    "\n",
    "model = Transliterator(encoder_hp=encoder_hp, decoder_hp=decoder_hp, attn = True)\n",
    "\n",
    "model.fit(train_pairs[:1000], validation_pairs, input_lang, output_lang, optimizer='adam', n_epochs = 1, learning_rate = 0.0003, teacher_forcing_ratio = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: xx5umkfg\n",
      "Sweep URL: https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/xx5umkfg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n8w8fde7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_num_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_type: gru\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_num_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_type: gru\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.8\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Aditya Patil\\Desktop\\IITM\\CS6910\\Assignment3\\wandb\\run-20230508_003253-n8w8fde7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/n8w8fde7' target=\"_blank\">bumbling-sweep-1</a></strong> to <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/xx5umkfg' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/xx5umkfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/xx5umkfg' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/sweeps/xx5umkfg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs20b004/CS6910_Assignment3/runs/n8w8fde7' target=\"_blank\">https://wandb.ai/cs20b004/CS6910_Assignment3/runs/n8w8fde7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random', #grid, random\n",
    "    'metric': {\n",
    "        'name': 'Validation Accuracy',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedding_size': {\n",
    "            'values': [64, 128]   }, \n",
    "        'hidden_size': {\n",
    "            'values': [128, 256, 384, 512]   },\n",
    "        'encoder_num_layers': { \n",
    "            'values': [1, 2, 3]   },\n",
    "        'encoder_type': {'values' : ['gru', 'lstm']},\n",
    "        'decoder_num_layers': {\n",
    "            'values': [1, 2, 3]   },\n",
    "        'decoder_type': {'values' : ['gru' ,'lstm']},\n",
    "        'learning_rate': {'values': [0.0001, 0.0006, 0.001, 0.0003]},\n",
    "        'teacher_forcing_ratio': {'values': [0.5, 0.6, 0.7, 0.8, 0.9]},\n",
    "        'optimizer' : {'values': ['adam', 'sgd']},\n",
    "        'epochs' : { 'values' : [10]}\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"CS6910_Assignment3\")\n",
    "\n",
    "def train():\n",
    "    config_defaults = {\n",
    "        'hidden_size': 256,\n",
    "        'encoder_num_layers': 1,\n",
    "        'encoder_type': 'gru',\n",
    "        'decoder_num_layers': 1,\n",
    "        'decoder_type': 'gru',\n",
    "        'learning_rate': 0.0001,\n",
    "        'teacher_forcing_ratio': 0.5,\n",
    "        'optimizer' : 'adam',\n",
    "        'epochs' : 5\n",
    "    }\n",
    "\n",
    "    run = wandb.init()\n",
    "    config = wandb.config\n",
    "    name_str = f\"nle_{wandb.config['encoder_num_layers']}_nld_{wandb.config['decoder_num_layers']}_lr_{wandb.config['learning_rate']}_eu_{wandb.config['encoder_type']}_du_{wandb.config['decoder_type']}\"\n",
    "    wandb.run.name = name_str\n",
    "    model = Transliterator(\n",
    "        encoder_hp={\n",
    "            'input_size': input_lang.n_letters,\n",
    "            'embedding_size' : config.embedding_size, \n",
    "            'hidden_size': config.hidden_size, \n",
    "            'num_layers': config.encoder_num_layers, \n",
    "            'type': config.encoder_type}, \n",
    "        decoder_hp={\n",
    "            'hidden_size': config.hidden_size, \n",
    "            'embedding_size': config.embedding_size,\n",
    "            'output_size': output_lang.n_letters, \n",
    "            'num_layers': config.decoder_num_layers, \n",
    "            'type': config.decoder_type}, \n",
    "        attn = False)\n",
    "    model.fit(train_pairs, validation_pairs, input_lang, output_lang, n_epochs = config.epochs, learning_rate = config.learning_rate, teacher_forcing_ratio = config.teacher_forcing_ratio, use_wanb = True)\n",
    "    run.finish()\n",
    "\n",
    "wandb.agent(sweep_id, train, count=5, project=\"CS6910_Assignment3\", entity=\"cs20b004\")\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
