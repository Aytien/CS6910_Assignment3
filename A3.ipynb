{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import wandb \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 40\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        # self.word2index = {}\n",
    "        # self.word2count = {}\n",
    "        # self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        # self.n_words = 2  # Count SOS and EOS\n",
    "        self.letter_index = {}\n",
    "        self.letter_count = {}\n",
    "        self.index_letter = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_letters = 2  # Count SOS and EOS\n",
    "\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # for word in sentence.split(' '):\n",
    "        #     self.addWord(word)\n",
    "        for letter in word:\n",
    "            self.add_letter(letter)\n",
    "\n",
    "    def add_letter(self, letter):\n",
    "        # if word not in self.word2index:\n",
    "        #     self.word2index[word] = self.n_words\n",
    "        #     self.word2count[word] = 1\n",
    "        #     self.index2word[self.n_words] = word\n",
    "        #     self.n_words += 1\n",
    "        # else:\n",
    "        #     self.word2count[word] += 1\n",
    "        if letter not in self.letter_index:\n",
    "            self.letter_index[letter] = self.n_letters\n",
    "            self.letter_count[letter] = 1\n",
    "            self.index_letter[self.n_letters] = letter\n",
    "            self.n_letters += 1\n",
    "        else:\n",
    "            self.letter_count[letter] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words(output_lang, type = 'train', reverse = False):\n",
    "    lines = open('aksharantar_sampled/%s/%s_%s.csv' % (output_lang,output_lang,type), encoding='utf-8').read().strip().split('\\n')\n",
    "    pairs = [[s for s in l.split(',')] for l in lines]\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "    else:\n",
    "        pairs = [list(p) for p in pairs]\n",
    "    return pairs\n",
    "\n",
    "def init_lang(lang1, lang2, type = 'train', reverse = False):\n",
    "    pairs = read_words(lang2, type, reverse)\n",
    "    input_lang = Lang(lang1)\n",
    "    output_lang = Lang(lang2)\n",
    "    char = 'a'\n",
    "    for i in range(26):\n",
    "        input_lang.add_letter(char)\n",
    "        char = chr(ord(char) + 1)\n",
    "    char = '\\u0900'\n",
    "    for i in range(128):\n",
    "        output_lang.add_letter(char)\n",
    "        char = chr(ord(char) + 1)\n",
    "\n",
    "    \n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX_LENGTH:  40\n"
     ]
    }
   ],
   "source": [
    "pairs = read_words('mar',type = 'train')\n",
    "for p in pairs:\n",
    "    MAX_LENGTH = max(MAX_LENGTH, len(p[1]))\n",
    "print(\"MAX_LENGTH: \", MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def indexesFromSentence(lang, sentence):\n",
    "#     return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "def word_to_index(lang, word):\n",
    "    return [lang.letter_index[letter] for letter in word]\n",
    "\n",
    "\n",
    "# def tensorFromSentence(lang, sentence):\n",
    "#     indexes = indexesFromSentence(lang, sentence)\n",
    "#     indexes.append(EOS_token)\n",
    "#     return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def word_to_tensor(lang, word):\n",
    "    indexes = word_to_index(lang, word)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "\n",
    "# def tensorsFromPair(pair):\n",
    "#     input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "#     target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "#     return (input_tensor, target_tensor)\n",
    "\n",
    "def pair_to_tensor(input_lang,output_lang, pair):\n",
    "    input_tensor = word_to_tensor(input_lang, pair[0])\n",
    "    target_tensor = word_to_tensor(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, type = 'gru', nonlinearity = 'tanh'):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.type = type\n",
    "        if self.type == 'gru':\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'lstm':\n",
    "            self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'rnn':\n",
    "            self.rnn = nn.RNN(hidden_size, hidden_size, num_layers = num_layers)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        if self.type == 'gru':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.type == 'lstm':\n",
    "            output, hidden = self.lstm(output, hidden)\n",
    "        elif self.type == 'rnn':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers , 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, nonlinearity = 'tanh', dropout_p = 0.1, type = 'gru'):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # self.dropout_p = dropout_p\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        # self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.num_layers = num_layers\n",
    "        self.type = type\n",
    "        if self.type == 'gru':\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'lstm':\n",
    "            self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'rnn':\n",
    "            self.rnn = nn.RNN(hidden_size, hidden_size, num_layers = num_layers)\n",
    "            \n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        # output = self.dropout(output)\n",
    "        output = F.relu(output)\n",
    "        if self.type == 'gru':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.type == 'lstm':\n",
    "            output, hidden = self.lstm(output, hidden)\n",
    "        elif self.type == 'rnn':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "        \n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, num_layers, nonlinearity = 'tanh', dropout_p=0.1, max_length=MAX_LENGTH, type = 'gru'):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.type = type\n",
    "        \n",
    "        if self.type == 'gru':\n",
    "            self.gru = nn.GRU(hidden_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'lstm':\n",
    "            self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers = num_layers)\n",
    "        elif self.type == 'rnn':\n",
    "            self.rnn = nn.RNN(hidden_size, hidden_size, num_layers = num_layers)\n",
    "\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        if self.type == 'gru':\n",
    "            output, hidden = self.gru(output, hidden)\n",
    "        elif self.type == 'lstm':\n",
    "            output, hidden = self.lstm(output, hidden)\n",
    "        elif self.type == 'rnn':\n",
    "            output, hidden = self.rnn(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(self.num_layers, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transliterator():\n",
    "    def __init__(self, encoder_hp, decoder_hp, attn = True):\n",
    "        self.encoder = EncoderRNN(encoder_hp['input_size'], encoder_hp['hidden_size'], encoder_hp['num_layers'], type = encoder_hp['type']).to(device)\n",
    "        self.attn = attn\n",
    "        if attn:\n",
    "            self.decoder = AttnDecoderRNN(decoder_hp['hidden_size'], decoder_hp['output_size'], decoder_hp['num_layers'], type = decoder_hp['type']).to(device)\n",
    "        else:\n",
    "            self.decoder = DecoderRNN(decoder_hp['hidden_size'], decoder_hp['output_size'], decoder_hp['num_layers'], type = decoder_hp['type']).to(device)\n",
    "\n",
    "    def train(self, input_tensor, target_tensor, max_length=MAX_LENGTH):\n",
    "        encoder_hidden = self.encoder.initHidden()\n",
    "\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "\n",
    "        input_length = input_tensor.size(0)\n",
    "        target_length = target_tensor.size(0)\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, self.encoder.hidden_size, device=device)\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = self.encoder(input_tensor[ei], encoder_hidden)\n",
    "            encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        decoder_hidden = encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)\n",
    "        decoded_word = ''\n",
    "        for i in range(self.decoder.num_layers - 1):\n",
    "            decoder_hidden = torch.cat((decoder_hidden, encoder_hidden[encoder_hidden.shape[0] - 1 - i].unsqueeze(0)), 0)\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < self.teacher_forcing_ratio else False\n",
    "        \n",
    "        if use_teacher_forcing:\n",
    "            for di in range(target_length):\n",
    "                if self.attn:\n",
    "                    decoder_output, decoder_hidden, decoder_attention = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  \n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "                decoded_word += self.output_lang.index_letter[topi.item()]\n",
    "                decoder_input = target_tensor[di]\n",
    "        else:\n",
    "            for di in range(target_length):\n",
    "                if self.attn:\n",
    "                    decoder_output, decoder_hidden, decoder_attention = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  \n",
    "                loss += self.criterion(decoder_output, target_tensor[di])\n",
    "                if decoder_input.item() == EOS_token:\n",
    "                    break\n",
    "                decoded_word += self.output_lang.index_letter[topi.item()]\n",
    "                \n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        \n",
    "        return loss.item() / target_length, decoded_word \n",
    "    \n",
    "    def fit( self, train_io_pair, val_io_pair, input_lang, output_lang, n_epochs,optimizer = 'adam', criterion = nn.NLLLoss(), print_every=1000, learning_rate=0.0001, teacher_forcing_ratio = 0.5):\n",
    "        self.input_lang = input_lang\n",
    "        self.output_lang = output_lang\n",
    "\n",
    "        if(optimizer == 'adam'):\n",
    "            self.encoder_optimizer = optim.Adam(self.encoder.parameters(), lr=learning_rate)\n",
    "            self.decoder_optimizer = optim.Adam(self.decoder.parameters(), lr=learning_rate)\n",
    "        elif(optimizer == 'sgd'):\n",
    "            self.encoder_optimizer = optim.SGD(self.encoder.parameters(), lr=learning_rate)\n",
    "            self.decoder_optimizer = optim.SGD(self.decoder.parameters(), lr=learning_rate)\n",
    "            \n",
    "        self.criterion = criterion\n",
    "        self.learning_rate = learning_rate\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.n_epochs = n_epochs\n",
    "        self.print_every = print_every\n",
    "        \n",
    "        self.all_losses = []\n",
    "        self.total_loss = 0\n",
    "\n",
    "        # for i in range(len(train_io_pair)):\n",
    "        #     train_io_pair[i] = pair_to_tensor(train_io_pair[i])\n",
    "        training_pairs = [pair_to_tensor(self.input_lang, self.output_lang, train_io_pair[i]) for i in range(len(train_io_pair))]\n",
    "        \n",
    "        for epc in range(1,n_epochs + 1):\n",
    "            correct = 0\n",
    "            for inp in range(len(train_io_pair)):\n",
    "                input_tensor = training_pairs[inp][0]\n",
    "                target_tensor = training_pairs[inp][1]\n",
    "                loss, decoder_output = self.train(input_tensor, target_tensor)\n",
    "                self.total_loss += loss\n",
    "                self.all_losses.append(loss)\n",
    "                if(decoder_output == train_io_pair[inp][1]):\n",
    "                    correct += 1\n",
    "            print(\"Epoch: \", epc, \" Loss: \", self.total_loss / len(train_io_pair),\"Training Accuracy: \", correct/len(train_io_pair),\"Validation Accuracy: \", self.eval(val_io_pair))\n",
    "            self.total_loss = 0\n",
    "\n",
    "    def eval(self, io_pairs):\n",
    "        correct = 0\n",
    "        for i in range(len(io_pairs)):\n",
    "            output, _ = self.predict(io_pairs[i][0])\n",
    "            if output == io_pairs[i][1]:\n",
    "                correct += 1\n",
    "        return correct / len(io_pairs)\n",
    "    \n",
    "    def predict(self, input_word, max_length=MAX_LENGTH):\n",
    "        with torch.no_grad():\n",
    "            input_tensor = word_to_tensor(self.input_lang, input_word)\n",
    "            input_length = input_tensor.size()[0]\n",
    "            encoder_hidden = self.encoder.initHidden()\n",
    "\n",
    "            encoder_outputs = torch.zeros(max_length, self.encoder.hidden_size, device=device)\n",
    "\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = self.encoder(input_tensor[ei],\n",
    "                                                        encoder_hidden)\n",
    "                encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "            decoder_hidden = encoder_hidden[encoder_hidden.shape[0] - 1].unsqueeze(0)\n",
    "            for i in range(self.decoder.num_layers - 1):\n",
    "                decoder_hidden = torch.cat((decoder_hidden, encoder_hidden[encoder_hidden.shape[0] - 1 - i].unsqueeze(0)), 0)\n",
    "\n",
    "            # decoded_words = []\n",
    "            decoded_word = ''\n",
    "            decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "            for di in range(max_length):\n",
    "                if self.attn:\n",
    "                    decoder_output, decoder_hidden, decoder_attention = self.decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                    decoder_attentions[di] = decoder_attention.data\n",
    "                else:\n",
    "                    decoder_output, decoder_hidden = self.decoder(\n",
    "                    decoder_input, decoder_hidden)\n",
    "                topv, topi = decoder_output.data.topk(1)\n",
    "                if topi.item() == EOS_token:\n",
    "                    # decoded_words.append('<EOS>')\n",
    "                    # decoded_word += '<EOS>'\n",
    "                    break\n",
    "                else:\n",
    "                    # decoded_words.append(output_lang.letter_index[topi.item()])\n",
    "                    decoded_word += self.output_lang.index_letter[topi.item()]\n",
    "                    # print(output_lang.index_letter[topi.item()])\n",
    "\n",
    "                decoder_input = topi.squeeze().detach()\n",
    "\n",
    "            return decoded_word, decoder_attentions[:di + 1]\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1  Loss:  1.278363156748678 Training Accuracy:  0.0368359375 Validation Accuracy:  0.1748046875\n",
      "Epoch:  2  Loss:  0.8726902751648318 Training Accuracy:  0.0878125 Validation Accuracy:  0.240478515625\n",
      "Epoch:  3  Loss:  0.7840190025630573 Training Accuracy:  0.1137109375 Validation Accuracy:  0.264404296875\n",
      "Epoch:  4  Loss:  0.7397998276464685 Training Accuracy:  0.127421875 Validation Accuracy:  0.25830078125\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, train_pairs = init_lang( 'eng','mar', type = 'train', reverse = False)\n",
    "validation_pairs = read_words('mar', type = 'valid')\n",
    "# print(input_lang.n_letters)\n",
    "encoder_hp = {'input_size': input_lang.n_letters, 'hidden_size': 256, 'num_layers': 3, 'type': 'gru'}\n",
    "decoder_hp = {'hidden_size': 256, 'output_size': output_lang.n_letters, 'num_layers': 2, 'type': 'gru'}\n",
    "\n",
    "model = Transliterator(encoder_hp=encoder_hp, decoder_hp=decoder_hp, attn = False)\n",
    "\n",
    "model.fit(train_pairs, validation_pairs, input_lang, output_lang, n_epochs = 5, learning_rate = 0.0003, teacher_forcing_ratio = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"aksharantar\", name=\"gru_attn\")\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random', #grid, random\n",
    "    'metric': {\n",
    "        'name': 'val_acc',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'encoder_hidden_size': {\n",
    "            'values': [64, 128, 256]   },\n",
    "        'encoder_num_layers': { \n",
    "            'values': [1, 2, 3]   },\n",
    "        'encoder_type': {'values' : ['gru', 'lstm', 'rnn']},\n",
    "        'decoder_hidden_size': {\n",
    "            'values': [64, 128, 256]   },\n",
    "        'decoder_num_layers': {\n",
    "            'values': [1, 2, 3]   },\n",
    "        'decoder_type': {'values' : ['gru', 'lstm', 'rnn']},\n",
    "        'learning_rate': {'values': [0.0001, 0.00001, 0.001, 0.0003]},\n",
    "        'teacher_forcing_ratio': {'values': [0.5, 0.6, 0.7, 0.8, 0.9]},\n",
    "        'optimizer' : {'values': ['adam', 'sgd']},\n",
    "        'epochs' : { 'values' : [5,10]}\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"aksharantar\")\n",
    "\n",
    "def train():\n",
    "    config_defaults = {\n",
    "        'encoder_hidden_size': 64,\n",
    "        'encoder_num_layers': 1,\n",
    "        'encoder_type': 'gru',\n",
    "        'decoder_hidden_size': 64,\n",
    "        'decoder_num_layers': 1,\n",
    "        'decoder_type': 'gru',\n",
    "        'learning_rate': 0.0001,\n",
    "        'teacher_forcing_ratio': 0.5,\n",
    "        'optimizer' : 'adam',\n",
    "        'epochs' : 5\n",
    "    }\n",
    "\n",
    "    run = wandb.init(project=\"aksharantar\", config=config_defaults)\n",
    "    config = wandb.config\n",
    "    name_str = config.encoder_type + '_' + config.decoder_type + '_' + str(config.encoder_hidden_size) + '_' + str(config.encoder_num_layers) + '_' + str(config.decoder_hidden_size) + '_' + str(config.decoder_num_layers) + '_' + str(config.learning_rate) + '_' + str(config.teacher_forcing_ratio) + '_' + config.optimizer + '_' + str(config.epochs)\n",
    "    wandb.run.name = name_str\n",
    "    model = Transliterator(\n",
    "        encoder_hp={\n",
    "            'input_size': input_lang.n_letters, \n",
    "            'hidden_size': config.encoder_hidden_size, \n",
    "            'num_layers': config.encoder_num_layers, \n",
    "            'type': config.encoder_type}, \n",
    "        decoder_hp={\n",
    "            'hidden_size': config.decoder_hidden_size, \n",
    "            'output_size': output_lang.n_letters, \n",
    "            'num_layers': config.decoder_num_layers, \n",
    "            'type': config.decoder_type}, \n",
    "        attn = True)\n",
    "    model.fit(train_pairs, validation_pairs, input_lang, output_lang, n_epochs = config.epochs, learning_rate = config.learning_rate, teacher_forcing_ratio = config.teacher_forcing_ratio)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
